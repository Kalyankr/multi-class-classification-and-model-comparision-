{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-class text classification modelcomparision & selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalyankr/multi-class-classification-and-model-comparision-/blob/master/Multi_class_text_classification_modelcomparision_%26_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TgMytFOL5QlR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from bs4 import BeautifulSoup\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdK2s7Eo81c3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwUiw0Qh88XO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"../content/stack-overflow-data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-F329m79PXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1YB-Zehp9txC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Will search for any Null post and tags. if prsent will drop them."
      ]
    },
    {
      "metadata": {
        "id": "Muu1qt7Q9Qoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(data[\"tags\"].isnull().sum())\n",
        "print(data[\"post\"].isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6H1LiSw-dNo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lets see how length \"post\" column looks exactly in the data "
      ]
    },
    {
      "metadata": {
        "id": "aQ3M552y9W3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(data[\"post\"].apply(lambda x:len(x.split(' '))).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iLy_HuG_Eg0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lets see the distribution of the labels in the data"
      ]
    },
    {
      "metadata": {
        "id": "amy6E82e-ofU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unique_tags=data[\"tags\"].unique()\n",
        "unique_tags\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bD5AU1zb_KEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "data.tags.value_counts().plot(kind=\"bar\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRBQ-Zf1AVs7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "looks like data distrubution was well balanced"
      ]
    },
    {
      "metadata": {
        "id": "8lCZyadsAME4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#look a few post and tag pairs.\n",
        "\n",
        "def print_plt(index):\n",
        "  example=data[data.index==index][[\"post\",'tags']].values[0]\n",
        "  if len(example) >0:\n",
        "    print(example[0])\n",
        "    print(\"Tag:\",example[1])\n",
        "print_plt(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1L9Cv_DzBXhL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we need to remove the Html tags, puntuations and stogwords"
      ]
    },
    {
      "metadata": {
        "id": "I1mDOH1ZBajB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "replace_by_space=re.compile(\"[/(){}\\[\\]\\|@,;]\")\n",
        "bad_symbols=re.compile('[^0-9a-z #+_]')\n",
        "Stopwords=set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsjM12amB76b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text=BeautifulSoup(text,\"lxml\").text # HTML decoding\n",
        "  text=text.lower() # lowercase text\n",
        "  text=replace_by_space.sub(\" \",text) # replace symbols by space in text\n",
        "  text=bad_symbols.sub(\"\",text) # delete sysmbols which are in bad_symbols from text\n",
        "  text=\" \".join(word for word in text.split() if word not in Stopwords) # remoce stop words\n",
        "  return  text\n",
        "\n",
        "\n",
        "data[\"post\"]=data[\"post\"].apply(clean_text)\n",
        "print_plt(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNHV1tSGDkSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data[\"post\"].apply(lambda x: len(x.split(\" \"))).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tI2_ArBaE2KI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train and test sets\n",
        "x=data.post\n",
        "y=data.tags\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sNw7FT8jEfyk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dIFk8Y6pF9hE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lets first go through Navie Bayes Classifier for multinomial model"
      ]
    },
    {
      "metadata": {
        "id": "9GeNPloKF_kk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "nb=Pipeline([(\"vect\",CountVectorizer()),\n",
        "            (\"tfidf\",TfidfTransformer()),\n",
        "            (\"clf\",MultinomialNB())])\n",
        "nb.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMtetWsMGJ4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "y_pred= nb.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdUwjtvGIY7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy %s\" %accuracy_score(y_pred,y_test))\n",
        "print(classification_report(y_test,y_pred,target_names=unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnT5OYT7JXpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "thats great naive bayes give 75% accuracy"
      ]
    },
    {
      "metadata": {
        "id": "VDzf3iUKNilt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Linear Support Vector Machine is widely regarded as one of the best text classification algorithms."
      ]
    },
    {
      "metadata": {
        "id": "XawJG4iGNgRz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd=Pipeline([(\"vect\",CountVectorizer()),\n",
        "            (\"tfidf\",TfidfTransformer()),\n",
        "            (\"clf\",SGDClassifier())])\n",
        "sgd.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTj-byThNzav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred= sgd.predict(x_test)\n",
        "print(\"Accuracy %s\" %accuracy_score(y_pred,y_test))\n",
        "print(classification_report(y_test,y_pred,target_names=unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUOSgoxzOCzH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great we went upto 80% now with 5% increase than Naive bayes"
      ]
    },
    {
      "metadata": {
        "id": "3buq77DSOOYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets go with **Logistic Regression**"
      ]
    },
    {
      "metadata": {
        "id": "ji-VRgwrN_7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lreg=Pipeline([(\"vect\",CountVectorizer()),\n",
        "            (\"tfidf\",TfidfTransformer()),\n",
        "            (\"clf\",LogisticRegression())])\n",
        "lreg.fit(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKIZ4OupOcUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred= lreg.predict(x_test)\n",
        "print(\"Accuracy %s\" %accuracy_score(y_pred,y_test))\n",
        "print(classification_report(y_test,y_pred,target_names=unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LYPo2JuU_P-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we got around 80%accuracy with logistic regression"
      ]
    },
    {
      "metadata": {
        "id": "cMPZeR9ZVNLv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we use Word2Vec and logistic regression. "
      ]
    },
    {
      "metadata": {
        "id": "f-_91vRLOgNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zn8FTKsLWY9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "list(islice(wv.vocab, 13030, 13050))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FduAnUAwVv2E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNx-RUCOWaDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "\n",
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QGiWKOmDWhGD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
        "y_pred = logreg.predict(X_test_word_average)\n",
        "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
        "print(classification_report(test.tags, y_pred,target_names=my_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcAMRbKKj-Ti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we got around 63% accuracy"
      ]
    },
    {
      "metadata": {
        "id": "G0wXZzx-WnpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Doc2vec and Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "ihho0yB2i4Yv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "you can find gensim doc2vec here https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "v-AmPaT8Wkma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "\n",
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.post, data.tags, random_state=0, test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1plQzJRaWtkW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePp1Fs37ceSC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors\n",
        "    \n",
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-YQaflCcfDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(train_vectors_dbow, y_train)\n",
        "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
        "y_pred = logreg.predict(test_vectors_dbow)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKWJKZ9GkEvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we got around 80%"
      ]
    },
    {
      "metadata": {
        "id": "jsiAaGEfifDT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets use BOW with Keras "
      ]
    },
    {
      "metadata": {
        "id": "SPst5soFcmYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "train_size = int(len(data) * .7)\n",
        "train_posts = data['post'][:train_size]\n",
        "train_tags = data['tags'][:train_size]\n",
        "\n",
        "test_posts = data['post'][train_size:]\n",
        "test_tags = data['tags'][train_size:]\n",
        "\n",
        "max_words = 1000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_posts) # only fit on train\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_posts)\n",
        "x_test = tokenize.texts_to_matrix(test_posts)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38wdfJurct_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dj7NXnLiaxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "now we got an accuracy of 79% "
      ]
    },
    {
      "metadata": {
        "id": "HgN66mRqe0Q5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}